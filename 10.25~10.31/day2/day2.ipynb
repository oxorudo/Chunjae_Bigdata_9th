{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유용한 vscode 단축키\n",
    "\n",
    "- ctrl + D : 같은 단어 연속 선택\n",
    "- 컨트ㄹ롤 + f : 현재 파일에서 단어 검색\n",
    "- 컨트롤 쉬프트 f : 현재 디렉토리으 ㅣ모든 파일에 대해 단어 검색\n",
    "- 컨트롤 b : explorer 창 온오프\n",
    "- alt 방향키위아래 : 선택한 코드블럭 위아래로 이동\n",
    "- shift alt 방향키위아래 : 선택한 코드 블럭 위아래로 복사\n",
    "- 컨트롤 ~ : 터미널 창 온오프\n",
    "- 컨트롤 쉬프트 5 : 터미널 창 split\n",
    "- 컨트롤 2 : 작업창 split\n",
    "- f1 : show command palette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DBconnector.py & settings.py\n",
    "\n",
    "- DB connector 모듈 및 공통 정보들을 각각의 파일형태로 나누어 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.connector  import DBConnector\n",
    "from settings import DB_SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SETTINGS['POSTGRES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "[(0, 1, 'Mary', 1880, 'F', 7065), (1, 2, 'Anna', 1880, 'F', 2604), (2, 3, 'Emma', 1880, 'F', 2003), (3, 4, 'Elizabeth', 1880, 'F', 1939), (4, 5, 'Minnie', 1880, 'F', 1746)]\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "from db.connector import DBConnector\n",
    "from settings import DB_SETTINGS\n",
    "\n",
    "db_connector = DBConnector(**DB_SETTINGS['POSTGRES'])\n",
    "\n",
    "with db_connector as connected:\n",
    "    conn = connected.conn\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('select * from lecture limit 5')\n",
    "\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\workspace\\\\Chunjae_Bigdata_9th\\\\10.25~10.31\\\\day2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"d:\\\\workspace\\\\Chunjae_Bigdata_9th\\\\10.25~10.31\\\\day2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. query.py\n",
    "\n",
    "    - 쿼리들은 파일로 관리하여 쉽게 호출할 수 있도록 작성\n",
    "\n",
    "    - 쿼리 내용 조회하는 부분을 class 내에 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import db.pgsql_query as postgresql_query\n",
    "from settings import DB_SETTINGS\n",
    "\n",
    "\n",
    "class DBConnector:\n",
    "    def __init__(self, host, database, user, password, port):\n",
    "        self.conn_params = dict(\n",
    "            host = host,\n",
    "            dbname = database,\n",
    "            user = user,\n",
    "            password = password,\n",
    "            port = port\n",
    "        )\n",
    "        self.connect = self.postgres_connect()\n",
    "        self.queries = postgresql_query.queries\n",
    "\n",
    "    def __enter__(self):\n",
    "        print(\"enter\")\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.conn.close()\n",
    "        print('exit')\n",
    "\n",
    "    def postgres_connect(self):\n",
    "        self.conn = psycopg2.connect(**self.conn_params)\n",
    "        return self\n",
    "    \n",
    "    def get_query(self, table_name):\n",
    "        try:\n",
    "            _query = self.queries[table_name]\n",
    "            return _query\n",
    "        except KeyError:\n",
    "            raise KeyError(f'{table_name} 키가 query에 존재하지 않습니다. 현재 있는 키 리스트 : {list(self.queries.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select * from lecture limit 5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_connector = DBConnector(**DB_SETTINGS['POSTGRES'])\n",
    "\n",
    "db_connector.get_query('lecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from lecture limit 5\n",
      "select * from tbl limit 5\n"
     ]
    }
   ],
   "source": [
    "from db.pgsql_query import queries\n",
    "\n",
    "for tbl in queries.keys():\n",
    "    db_connector = DBConnector(**DB_SETTINGS['POSTGRES'])\n",
    "    _query = db_connector.get_query(tbl)\n",
    "    print(_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. extract.py\n",
    "\n",
    "- 쿼리를 받아 DB에 조회하여 결과를 pandas dataframe으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.connector import DBConnector\n",
    "from settings import DB_SETTINGS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "exit\n",
      "   index  id       name  year gender  count\n",
      "0      0   1       Mary  1880      F   7065\n",
      "1      1   2       Anna  1880      F   2604\n",
      "2      2   3       Emma  1880      F   2003\n",
      "3      3   4  Elizabeth  1880      F   1939\n",
      "4      4   5     Minnie  1880      F   1746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10736\\2502543534.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(_query, conn)\n"
     ]
    }
   ],
   "source": [
    "db_connector = DBConnector(**DB_SETTINGS['POSTGRES'])\n",
    "\n",
    "with db_connector as connected:\n",
    "    _query = connected.get_query('lecture')\n",
    "    conn = connected.conn\n",
    "    df = pd.read_sql(_query, conn)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract\n",
    "\n",
    "def extractor(db_connector, table_name):\n",
    "\n",
    "    with db_connector as connected:\n",
    "        try:\n",
    "            _query = connected.get_query(table_name)\n",
    "            conn = connected.conn\n",
    "            df = pd.read_sql(_query, conn)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f'Extract MSG : {e}')\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10736\\528373740.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(_query, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>gender</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mary</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Anna</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>2604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Emma</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Minnie</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  id       name  year gender  count\n",
       "0      0   1       Mary  1880      F   7065\n",
       "1      1   2       Anna  1880      F   2604\n",
       "2      2   3       Emma  1880      F   2003\n",
       "3      3   4  Elizabeth  1880      F   1939\n",
       "4      4   5     Minnie  1880      F   1746"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_connector = DBConnector(**DB_SETTINGS['POSTGRES'])\n",
    "\n",
    "return_extractor = extractor(db_connector, 'lecture')\n",
    "return_extractor.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. transform.py\n",
    "\n",
    "- Batch 날짜(년/월/일)별 저장 경로 생성 및 해당 경로 이하에 dataframe 저장\n",
    "- 이행 환경에 따라 다르게 구성될 수 있음\n",
    "    - Database -> Stagins Server -> Cloud / Database\n",
    "    - Database -- Directory Connection -> Cloud / Database\n",
    "\n",
    "- 목적지 Database의 성격에 따라 추가적인 처리 함수가 포함될 수 있음\n",
    "    - Data Lake -> 거의 가공 없이 이행\n",
    "    - Data Warehouse -> 결측치/공백 등 간단한 전처리를 거쳐 이행\n",
    "    - Data Mart -> Group by/filter 등 성격에 맞는 데이터 처리를 거쳐 이행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 저장 경로 생성\n",
    "- Database 이름 / Table 이름 / yyyy={} / mm{} / dd={} / {table_name}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "exit\n",
      "   index  id       name  year gender  count\n",
      "0      0   1       Mary  1880      F   7065\n",
      "1      1   2       Anna  1880      F   2604\n",
      "2      2   3       Emma  1880      F   2003\n",
      "3      3   4  Elizabeth  1880      F   1939\n",
      "4      4   5     Minnie  1880      F   1746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\Chunjae_Bigdata_9th\\10.25~10.31\\day2\\pipeline\\extract.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(_query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from db.connector import DBConnector\n",
    "from settings import DB_SETTINGS\n",
    "from pipeline.extract import extractor\n",
    "\n",
    "db_connector = DBConnector(**DB_SETTINGS['POSTGRES'])\n",
    "table_name = 'lecture'\n",
    "\n",
    "return_extractor = extractor(db_connector, table_name)\n",
    "\n",
    "# return_extractor.head()\n",
    "\n",
    "if isinstance(return_extractor, pd.DataFrame):\n",
    "    print(return_extractor.head())\n",
    "else:\n",
    "    print('데이터를 가져오지 못함')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2024', '10', '28')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch 날짜 설정\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "batch_date = datetime.now()\n",
    "format_date = batch_date.strftime(r'%Y%m%d')\n",
    "\n",
    "_y = format_date[:4]\n",
    "_m = format_date[4:6]\n",
    "_d = format_date[6:]\n",
    "\n",
    "_y, _m, _d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' 2024', ' 10', ' 28')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{batch_date: %Y}', f'{batch_date: %m}', f'{batch_date: %d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\workspace\\\\Chunjae_Bigdata_9th\\\\10.25~10.31\\\\day2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\workspace\\\\Chunjae_Bigdata_9th\\\\10.25~10.31\\\\day2\\\\temp_storage\\\\postgres\\\\lecture'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "temp_path = 'd:\\\\workspace\\\\Chunjae_Bigdata_9th\\\\10.25~10.31\\\\day2\\\\temp_storage'\n",
    "\n",
    "_path = os.path.join(temp_path, 'postgres', 'lecture')\n",
    "_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "batch_date = datetime.now().strftime(r\"%Y%m%d\")\n",
    "temp_path = 'd:\\\\workspace\\\\Chunjae_Bigdata_9th\\\\10.25~10.31\\\\day2\\\\temp_storage'\n",
    "\n",
    "def create_path(temp_path, batch_date):\n",
    "\n",
    "    _y = batch_date[:4]\n",
    "    _m = batch_date[4:6]\n",
    "    _d = batch_date[6:]\n",
    "\n",
    "    return _path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) pandas dataframe을 csv, parquet 형태로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 폴더 생성\n",
    "\n",
    "path = create_path(temp_path, batch_date)\n",
    "\n",
    "os.makedirs(path, mode=777, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv format\n",
    "save_path = os.path.join(path, 'lecture.csv')\n",
    "save_path\n",
    "\n",
    "df.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json format\n",
    "\n",
    "save_path = os.path.join(path, 'lecture.json')\n",
    "\n",
    "df.to_json(save_path, orient='records', indent=4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet format\n",
    "\n",
    "save_path = os.path.join(path, 'lecture.parquet')\n",
    "\n",
    "df.to_parquet(save_path, engine = 'pyarrow', compression = 'gzip', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(df, path, table_name):\n",
    "    if len(df) > 0:\n",
    "        os.makedirs(path, mode=777)\n",
    "        save_path = os.path.join(path, f'{table_name}.csv')\n",
    "\n",
    "        df.to_csv(save_path)\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        print('empty file')\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_to_file(df, path, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장 경로 생성 + DataFrame 저장 함수 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer(create_path + save_to_file) 함수\n",
    "\n",
    "def transformer(temp_path, batch_date, df, table_name):\n",
    "    path = create_path(temp_path, batch_date)\n",
    "    res = save_to_file(df, path, table_name)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(temp_path, batch_date, df, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.connector import DBConnector\n",
    "from settings import DB_SETTINGS, TEMP_PATH\n",
    "from pipeline.extract import extractor\n",
    "from pipeline.transform import transformer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "db_connector = DBConnector(**DB_SETTINGS['POSTGRES'])\n",
    "table_name = 'lecture'\n",
    "batch_date = datetime.now().strftime(r'%Y%m%d')\n",
    "# print(batch_date)\n",
    "\n",
    "return_extractor = extractor(db_connector, table_name)\n",
    "return_extractor\n",
    "\n",
    "if return_extractor is not None and not return_extractor.empty:\n",
    "    return_transformer = transformer(TEMP_PATH, batch_date, return_extractor, table_name)\n",
    "else:\n",
    "    print('dataframe이 비었거나 데이터 추출에 실패했습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. load.py\n",
    "\n",
    "- 저장된 파일을 특정한 저장소에 적재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) pandas to_sql() 메서드를 활용한 테이블 적재(local file -> database)\n",
    "\n",
    "- PANDAS > to_sql\n",
    "    - [dataframe].to_sql(name = '테이블 이름', con = 'sqlalchemy connection', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = 'postgresql'\n",
    "user = 'postgres'\n",
    "password = 'Tkkwak0419?!'\n",
    "host = '127.0.0.1'\n",
    "port = '5432'\n",
    "database = 'postgres'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(postgresql://postgres:***@127.0.0.1:5432/postgres)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = create_engine(f'{engine}://{user}:{password}@{host}:{port}/{database}')\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>gender</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mary</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>7065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Anna</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>2604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Emma</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Minnie</td>\n",
       "      <td>1880</td>\n",
       "      <td>F</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       name  year gender  count\n",
       "0   1       Mary  1880      F   7065\n",
       "1   2       Anna  1880      F   2604\n",
       "2   3       Emma  1880      F   2003\n",
       "3   4  Elizabeth  1880      F   1939\n",
       "4   5     Minnie  1880      F   1746"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./dataset/data-01/names.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      2000 non-null   int64 \n",
      " 1   name    2000 non-null   object\n",
      " 2   year    2000 non-null   int64 \n",
      " 3   gender  2000 non-null   object\n",
      " 4   count   2000 non-null   int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 78.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql(name='point', con=db, if_exists='replace')\n",
    "                                        # 'fail' : 테이블이 존재하면 실패\n",
    "                                        # 'append' : 테이블이 존재하면 아래 row에 추가\n",
    "                                        # 'replace' : 테이블이 존재하면 명령 내링 테이블로 데이터 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(db_connector, df, table_name):\n",
    "    with db_connector as connected:\n",
    "        try:\n",
    "            orm_conn = connected.orm_conn\n",
    "            df.to_sql(name=table_name, con=orm_conn, if_exists='replace')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f'loader Error MSG: {e}')\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\Chunjae_Bigdata_9th\\10.25~10.31\\day2\\pipeline\\extract.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(_query, conn)\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'd:\\\\workspace\\\\Chunjae_Bigdata_9th\\\\10.25~10.31\\\\day2\\\\temp_storage\\\\lecture\\\\yyyy=2024\\\\mm = 10\\\\dd=28'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# return_extractor\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_extractor\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 18\u001b[0m     return_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEMP_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(return_transformer)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_transformer:\n",
      "File \u001b[1;32md:\\workspace\\Chunjae_Bigdata_9th\\10.25~10.31\\day2\\pipeline\\transform.py:5\u001b[0m, in \u001b[0;36mtransformer\u001b[1;34m(temp_path, batch_date, df, table_name)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransformer\u001b[39m(temp_path, batch_date, df, table_name):\n\u001b[0;32m      4\u001b[0m     path \u001b[38;5;241m=\u001b[39m create_path(temp_path, batch_date, table_name)\n\u001b[1;32m----> 5\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43msave_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32md:\\workspace\\Chunjae_Bigdata_9th\\10.25~10.31\\day2\\pipeline\\transform.py:24\u001b[0m, in \u001b[0;36msave_to_file\u001b[1;34m(df, path, table_name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_to_file\u001b[39m(df, path, table_name):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 24\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m777\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m         save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m         df\u001b[38;5;241m.\u001b[39mto_csv(save_path)\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] 파일이 이미 있으므로 만들 수 없습니다: 'd:\\\\workspace\\\\Chunjae_Bigdata_9th\\\\10.25~10.31\\\\day2\\\\temp_storage\\\\lecture\\\\yyyy=2024\\\\mm = 10\\\\dd=28'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from db.connector import DBConnector\n",
    "from settings import DB_SETTINGS, TEMP_PATH\n",
    "from pipeline.extract import extractor\n",
    "from pipeline.load import loader\n",
    "from pipeline.transform import transformer\n",
    "from datetime import datetime\n",
    "\n",
    "db_connector = DBConnector(**DB_SETTINGS['POSTGRES'])\n",
    "table_name = 'lecture'\n",
    "batch_date = datetime.now().strftime(r'%Y%m%d')\n",
    "\n",
    "return_extractor = extractor(db_connector, table_name)\n",
    "# return_extractor\n",
    "\n",
    "if return_extractor is not None and not return_extractor.empty:\n",
    "    return_transformer = transformer(TEMP_PATH, batch_date, return_extractor, table_name)\n",
    "\n",
    "\n",
    "\n",
    "if return_transformer:\n",
    "    return_loader = loader(db_connector, return_transformer, table_name)\n",
    "\n",
    "return_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. remove.py\n",
    "\n",
    "\n",
    "- 임시 저장된 파일을 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "from settings import TEMP_PATH\n",
    "\n",
    "shutil.rmtree(TEMP_PATH)\n",
    "\n",
    "os.makedirs(TEMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover\n",
    "\n",
    "def remover(path):\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        os.makedirs(path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'Remover Error MSG : {e}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter\n",
      "exit\n",
      "enter\n",
      "loader Error MSG: A column with name 'level_0' is already present in table 'lecture'.\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "from db.connector import DBConnector\n",
    "from settings import DB_SETTINGS, TEMP_PATH\n",
    "from pipeline.extract import extractor\n",
    "from pipeline.transform import transformer\n",
    "from pipeline.load import loader\n",
    "from pipeline.remove import remover\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "db_connector = DBConnector(**DB_SETTINGS['POSTGRES'])\n",
    "table_name = 'lecture'\n",
    "batch_date = datetime.now().strftime(r'%Y%m%d')\n",
    "\n",
    "return_extractor = extractor(db_connector, table_name)\n",
    "# return_extractor\n",
    "\n",
    "if return_extractor is not None and not return_extractor.empty:\n",
    "    return_transformer = transformer(TEMP_PATH, batch_date, return_extractor, table_name)\n",
    "\n",
    "if return_transformer:\n",
    "    return_loader = loader(db_connector, return_extractor, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remover(TEMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'bb']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = {'aa' : 1, 'bb' : 2}\n",
    "list (aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lecture': 'select * from lecture limit 5',\n",
       " 'tbl': 'select * from tbl limit 5'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = __import__(\"db.pgsql_query\", fromlist=[\"\"])\n",
    "queries.queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NewYork', 1)]\n",
      "[('NewYork', 1), ('London', 20)]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "class CargoShip:\n",
    "    def __init__(self, capacity):\n",
    "        self.cargo = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def unload(self, port):\n",
    "        port_list = [p[0] for p in self.cargo]\n",
    "        if port in port_list:\n",
    "            unloaded = [i for i in self.cargo if i[0] == port]\n",
    "            return unloaded\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    def can_depart(self):\n",
    "        _depart = True if sum([i[1] for i in self.cargo]) <- self.capacity else False\n",
    "        return _depart\n",
    "    \n",
    "    def load(self, new_cargo):\n",
    "        self.cargo: list = new_cargo\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ship = CargoShip(20)\n",
    "    ship.load([('NewYork', 1), ('London', 20)])\n",
    "    print(ship.unload('NewYork'))\n",
    "    print(ship.cargo)\n",
    "    print(ship.can_depart())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"name\": \"eggs\", \"price\": 1},{\"name\": \"rice\", \"price\": 4.04},{\"name\": \"coffee\", \"price\": 9.99}']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "aa = [{'name' : 'eggs', 'price' : 1}, {'name' : 'coffee', 'price' : 9.99}, {'name' : 'rice', 'price' : 4.04}]\n",
    "\n",
    "sorted_items = sorted(aa, key = lambda x: (x['price'], x['name']))\n",
    "\n",
    "joined_json = ','.join([json.dumps(i) for i in sorted_items])\n",
    "\n",
    "ret = ','.join([json.dumps(i) for i in sorted_items])\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def sort_by_price_ascending(json_string):\n",
    "    json_string = eval(json_string)\n",
    "\n",
    "    sorted_json = sorted(json_string, key=lambda x: (x['price'], x['name']))\n",
    "\n",
    "    joined_json = ','.join([json.dumps(i) for i in sorted_json])\n",
    "    print(str(joined_json))\n",
    "    final = '[' + joined_json + ']'\n",
    "\n",
    "    return final.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"eggs\", \"price\": 1},{\"name\": \"rice\", \"price\": 4.04},{\"name\": \"coffee\", \"price\": 9.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[{\"name\":\"eggs\",\"price\":1},{\"name\":\"rice\",\"price\":4.04},{\"name\":\"coffee\",\"price\":9.99}]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = sort_by_price_ascending('{\"name\": \"eggs\", \"price\": 1},{\"name\": \"rice\", \"price\": 4.04},{\"name\": \"coffee\", \"price\": 9.99}')\n",
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010/02/20\n",
      "09/01/1994\n",
      "10-09-1996\n",
      "['20100220', '19940109', '19961009']\n"
     ]
    }
   ],
   "source": [
    "# 날짜 바꾸기\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_date_format(dates):\n",
    "    date = [d for d in dates if '/' in d or '-' in d]\n",
    "    date_list = []\n",
    "\n",
    "    for date_str in dates:\n",
    "        if re.match(r'\\d{4}/(0[1-9]|1[0-2])/(0[1-9]|1[0-9]|2[0-9]|3[0-1])', date_str):\n",
    "            print(date_str)\n",
    "            transformed_date = datetime.strptime(date_str, '%Y/%m/%d').strftime('%Y%m%d')\n",
    "            date_list.append(transformed_date)\n",
    "        elif re.match(r'(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}', date_str):\n",
    "            print(date_str)\n",
    "            transformed_date = datetime.strptime(date_str, '%d/%m/%Y').strftime('%Y%m%d')\n",
    "            date_list.append(transformed_date)\n",
    "        elif re.match(r'(0[1-9]|1[0-2])-(0[1-9]|1[0-9]|2[0-9]|3[0-1])-\\d{4}', date_str):\n",
    "            print(date_str)\n",
    "            transformed_date = datetime.strptime(date_str, '%m-%d-%Y').strftime(\"%Y%m%d\")\n",
    "            date_list.append(transformed_date)\n",
    "\n",
    "    return date_list\n",
    "\n",
    "\n",
    "dates = ['2010/02/20', '09/01/1994', '10-09-1996', '20210221']\n",
    "sformed_dates = transform_date_format(dates)\n",
    "print(sformed_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200220\n",
      "19940109\n",
      "19960910\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def match_pattern(date_str):\n",
    "\n",
    "    pattern_list = [\n",
    "        r'\\d{4}/(0[1-9]|1[0-2])/(0[1-9]|1[0-9]|2[0-9]|3[0-1])',\n",
    "        r'(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}',\n",
    "        r'(0[1-9]|1[0-2])-(0[1-9]|1[0-9]|2[0-9]|3[0-1])-\\d{4}'\n",
    "    ]\n",
    "    idx = [bool(re.search(ptn, date_str)) for ptn in pattern_list].index(True)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def transform_date_format(dates):\n",
    "    \n",
    "    dates = [d for d in dates if '/' in d or '-' in d]\n",
    "    dates = [d for d in dates if len(d) == 10]\n",
    "    date_list = []\n",
    "\n",
    "    for date_str in dates:\n",
    "        pattern_idx = match_pattern(date_str)\n",
    "\n",
    "        if pattern_idx == 0:\n",
    "            date_list.append(datetime.strptime(date_str, '%Y/%m/%d').strftime('%Y%m%d'))\n",
    "        elif pattern_idx == 1:\n",
    "            date_list.append(datetime.strptime(date_str, '%d/%m/%Y').strftime('%Y%m%d'))\n",
    "        elif pattern_idx == 2:\n",
    "            date_list.append(datetime.strptime(date_str, '%d-%m-%Y').strftime('%Y%m%d'))\n",
    "\n",
    "    return date_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dates = transform_date_format(['2020/02/20', '09/01/1994', '10-09-1996', '20210221'])\n",
    "    print(*dates, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200220\n",
      "19940109\n",
      "19960910\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def transform_date_format(dates):\n",
    "\n",
    "    dates = [d for d in dates if '/' in d or '-' in d]\n",
    "    dates = [d for d in dates if len(d) == 10]\n",
    "    date_list = []\n",
    "\n",
    "    for date_str in dates:\n",
    "        if re.match(r'\\d{4}/(0[1-9]|1[0-2])/(0[1-9]|1[0-9]|2[0-9]|3[0-1])', date_str):\n",
    "            date_list.append(datetime.strptime(date_str, '%Y/%m/%d').strftime('%Y%m%d'))\n",
    "        if re.match(r'(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}', date_str):\n",
    "            date_list.append(datetime.strptime(date_str, '%d/%m/%Y').strftime('%Y%m%d'))\n",
    "        if re.match(r'(0[1-9]|1[0-2])-(0[1-9]|1[0-9]|2[0-9]|3[0-1])-\\d{4}', date_str):\n",
    "            date_list.append(datetime.strptime(date_str, '%d-%m-%Y').strftime('%Y%m%d'))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return date_list\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dates = transform_date_format(['2020/02/20', '09/01/1994', '10-09-1996', '20210221'])\n",
    "    print(*dates, sep='\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200220\n",
      "19940109\n",
      "19960910\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def transform_date_format(dates):\n",
    "\n",
    "    dates = [d for d in dates if '/' in d or '-' in d]\n",
    "    dates = [d for d in dates if len(d) == 10]\n",
    "    date_list = []\n",
    "\n",
    "    for date_str in dates:\n",
    "        if re.match(r'\\d{4}/(0[1-9]|1[0-2])/(0[1-9]|1[0-9]|2[0-9]|3[0-1])', date_str):\n",
    "            transformed_date = datetime.strptime(date_str, '%Y/%m/%d').strftime('%Y%m%d')\n",
    "        if re.match(r'(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}', date_str):\n",
    "            transformed_date = datetime.strptime(date_str, '%d/%m/%Y').strftime('%Y%m%d')\n",
    "        if re.match(r'(0[1-9]|1[0-2])-(0[1-9]|1[0-9]|2[0-9]|3[0-1])-\\d{4}', date_str):\n",
    "            transformed_date = datetime.strptime(date_str, '%d-%m-%Y').strftime('%Y%m%d')\n",
    "        \n",
    "        date_list.append(transformed_date)\n",
    "\n",
    "    return date_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dates = transform_date_format(['2020/02/20', '09/01/1994', '10-09-1996', '20210221'])\n",
    "    print(*dates, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def transform_date_format(dates):\n",
    "\n",
    "    dates = [d for d in dates if '/' in d or '-' in d]\n",
    "    dates = [d for d in dates if len(d) == 10]\n",
    "    date_list = []\n",
    "\n",
    "    for date_str in dates:\n",
    "        if re.match(r'\\d{4}/(0[1-9]|1[0-2])/(0[1-9]|1[0-9]|2[0-9]|3[0-1])', date_str):\n",
    "            transformed_date = datetime.strptime(date_str, '%Y/%m/%d').strftime('%Y%m%d')\n",
    "            date_list.append(transformed_date)\n",
    "        elif re.match(r'(0[1-9]|1[0-9]|2[0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}', date_str):\n",
    "            transformed_date = datetime.strptime(date_str, '%d/%m/%Y').strftime('%Y%m%d')\n",
    "            date_list.append(transformed_date)\n",
    "        elif re.match(r'(0[1-9]|1[0-2])-(0[1-9]|1[0-9]|2[0-9]|3[0-1])-\\d{4}', date_str):\n",
    "            transformed_date = datetime.strptime(date_str, '%d-%m-%Y').strftime('%Y%m%d')\n",
    "            date_list.append(transformed_date)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return date_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controller.py\n",
    "    - extractor, transformer 등 개별 모듈들에 대하여 순서대로 명령을 내려주는 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller(): \n",
    "    \"\"\"\n",
    "    \n",
    "    1. DBConnector >> DB Connector 생성\n",
    "    2. postgresql_query >> queries 에서 테이블 이름 목록(table_list) 받아오기\n",
    "        ex) \n",
    "            for tbl in table_list:\n",
    "    3. extract >> DB 조회 후 DataFrame 형태로 변환\n",
    "    4. transform >> 저장 경로 생성 후 임시 저장 디렉토리 아래에 dataframe 저장\n",
    "    5. load >> 저장소에 dataframe 파일 저장\n",
    "    6. remove >> 저장이 끝난 후 임시 저장 디렉토리 삭제\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.connector import DBConnector\n",
    "from settings import DB_SETTINGS, TEMP_PATH\n",
    "from pipeline.extract import extractor\n",
    "from pipeline.transform import transformer\n",
    "from pipeline.load import loader\n",
    "from pipeline.remove import remover\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
